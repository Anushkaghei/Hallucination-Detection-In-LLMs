{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anushkaghei/Hallucination-Detection-In-LLMs/blob/main/Multi_agent_system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install xmltodict\n",
        "!pip install langchain_community\n",
        "!pip install -q -U google-generativeai langchain-google-genai\n",
        "%pip install --upgrade --quiet  langchain-google-genai pillow"
      ],
      "metadata": {
        "id": "ePa09emn3e66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "import textwrap\n",
        "import google.generativeai as genai\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "from google.colab import userdata\n",
        "#from langchain_community.retrievers import PubMedRetriever\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.retrievers import PubMedRetriever\n",
        "from langchain.agents.format_scratchpad.openai_tools import (format_to_openai_tool_messages,)\n",
        "from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser\n",
        "from langchain.agents import AgentExecutor\n",
        "from langchain.agents import tool\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI"
      ],
      "metadata": {
        "id": "GucqSNQ-ygeX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Provide your Google API Key\")"
      ],
      "metadata": {
        "id": "iYRK8D9RD09J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
        "result = llm.invoke(\"Write a comprehensive report on it: Why is the Wernickie Area considered so important? It is such a wide area with limited study on it.\")"
      ],
      "metadata": {
        "id": "r-mOD5wAD83N"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_markdown(text):\n",
        "  text = text.replace('â€¢', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ],
      "metadata": {
        "id": "4mDPDYCjw9LV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_markdown(result.content)"
      ],
      "metadata": {
        "id": "nV7ci7-B1AXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = PubMedRetriever(\n",
        "    index_name=\"my_index\",\n",
        "    batch_size=10,\n",
        "    retrieve_kwargs={\"top_k\": 10},\n",
        ")"
      ],
      "metadata": {
        "id": "fDo2GiWFB-Ur"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = genai.GenerativeModel('gemini-pro')\n",
        "\n",
        "def augmented_generator(query):\n",
        "  # 1. Retrieve relevant documents using PubMed Retriever\n",
        "  documents = retriever.get_relevant_documents(query)\n",
        "\n",
        "  prompts = [\"Answer the question based on the given context: \" + query + \"\"]\n",
        "  for doc in documents:\n",
        "    # Get title from metadata (assuming a 'title' key exists)\n",
        "    title = doc.metadata.get(\"title\", \"\")  # Handle potential missing key\n",
        "    prompts.append(title + \": \" + doc.page_content)\n",
        "\n",
        "  augmented_text = model2.generate_content(prompts)\n",
        "\n",
        "  return augmented_text\n",
        "\n",
        "query = \"Why is the Wernickie Area considered so important? It is such a wide area with limited study on it.\"\n",
        "augmented_text = augmented_generator(query)\n",
        "\n",
        "x = to_markdown(augmented_text.text)\n",
        "\n",
        "x"
      ],
      "metadata": {
        "id": "x1kJV3zN1OER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score sacrebleu transformers"
      ],
      "metadata": {
        "id": "QSO9Dv1dKyk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert_score"
      ],
      "metadata": {
        "id": "G3s0YbFBX03n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install questeval"
      ],
      "metadata": {
        "id": "w972j_Svo18Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer\n",
        "from sacrebleu import BLEU  # BLEU score from sacrebleu library\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import bert_score"
      ],
      "metadata": {
        "id": "8ThcifWVXceR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the two text variables\n",
        "x = result.content\n",
        "y = augmented_text.text\n",
        "\n",
        "# Calculate ROUGE-L score\n",
        "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "rouge_l_score = scorer.score(x, y)['rougeL'].fmeasure\n",
        "print(f\"ROUGE-L Score: {rouge_l_score:.4f}\")\n",
        "\n",
        "# Calculate BERTScore\n",
        "bert_scorer = bert_score.score([x], [y], lang='en', model_type='bert-base-uncased')\n",
        "bert_score_value = bert_scorer[2].item()\n",
        "print(f\"BERTScore: {bert_score_value:.4f}\")\n",
        "\n",
        "# Calculate BLEU score\n",
        "bleu_scorer = BLEU()\n",
        "bleu_score = bleu_scorer.corpus_score([x], [[y]]).score\n",
        "print(f\"BLEU Score: {bleu_score:.4f}\")\n",
        "\n",
        "average_score = (rouge_l_score + bert_score_value) / 2\n",
        "print(f\"Average Score: {average_score:.4f}\")\n",
        "\n",
        "# Flag as hallucination if the average score is less than 0.5\n",
        "if average_score < 0.5:\n",
        "    print(\"Flagged as hallucination.\")\n",
        "else:\n",
        "    print(\"Not flagged as hallucination.\")\n"
      ],
      "metadata": {
        "id": "UFF6JcZkY1Fb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from rouge_score import rouge_scorer\n",
        "import bert_score\n",
        "from sacrebleu import BLEU\n",
        "\n",
        "rouge_l_scores = []\n",
        "bert_scores = []\n",
        "bleu_scores = []\n",
        "\n",
        "# Read the CSV file\n",
        "data = pd.read_csv('/content/drive/MyDrive/merged_final_dataset.csv')\n",
        "\n",
        "# Initialize scorers\n",
        "rouge_scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "bleu_scorer = BLEU()\n",
        "\n",
        "# Iterate over rows\n",
        "for _, row in data.iterrows():\n",
        "    input_text = row['input']\n",
        "    expected_output = row['output']\n",
        "\n",
        "    # Generate outputs from LLMs x and y\n",
        "    output_x = llm.invoke('Answer the question based on the given context: ' + input_text).content\n",
        "    output_y = augmented_generator(input_text).text\n",
        "\n",
        "    # Calculate scores\n",
        "    rouge_l_score_x = rouge_scorer.score(output_x, expected_output)['rougeL'].fmeasure\n",
        "    rouge_l_score_y = rouge_scorer.score(output_y, expected_output)['rougeL'].fmeasure\n",
        "\n",
        "    bert_scorer_x = bert_score.score([output_x], [expected_output], lang='en', model_type='bert-base-uncased')\n",
        "    bert_score_x = bert_scorer_x[2].item()\n",
        "    bert_scorer_y = bert_score.score([output_y], [expected_output], lang='en', model_type='bert-base-uncased')\n",
        "    bert_score_y = bert_scorer_y[2].item()\n",
        "\n",
        "    bleu_score_x = bleu_scorer.corpus_score([output_x], [[expected_output]]).score\n",
        "    bleu_score_y = bleu_scorer.corpus_score([output_y], [[expected_output]]).score\n",
        "\n",
        "    # Print scores\n",
        "    print(f\"Input: {input_text}\")\n",
        "    print(f\"Expected Output: {expected_output}\")\n",
        "    print(f\"Output x: {output_x}\")\n",
        "    print(f\"Output y: {output_y}\")\n",
        "    print(f\"ROUGE-L Score x: {rouge_l_score_x:.4f}, ROUGE-L Score y: {rouge_l_score_y:.4f}\")\n",
        "    print(f\"BERTScore x: {bert_score_x:.4f}, BERTScore y: {bert_score_y:.4f}\")\n",
        "    print(f\"BLEU Score x: {bleu_score_x:.4f}, BLEU Score y: {bleu_score_y:.4f}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    rouge_l_scores.append(rouge_l_score_x)\n",
        "    bert_scores.append(bert_score_x)\n",
        "    bleu_scores.append(bleu_score_x)\n",
        "\n",
        "average_rouge_l_score = sum(rouge_l_scores) / len(rouge_l_scores)\n",
        "print(f\"Average ROUGE-L Score: {average_rouge_l_score:.4f}\")\n",
        "\n",
        "average_bert_score = sum(bert_scores) / len(bert_scores)\n",
        "print(f\"Average BERT Score: {average_bert_score:.4f}\")\n",
        "\n",
        "average_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
        "print(f\"Average BLEU Score: {average_bleu_score:.4f}\")\n"
      ],
      "metadata": {
        "id": "jOKTMq1Dh3vl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}